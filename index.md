## UW NLP Capstone: Team BallmerNLP

### Blog Post \#1

Our team, BallmerNLP, is comprised of Nelson Liu, Mathew Luo, and Deric Pang.
Our capstone will be hosted at
[https://github.com/dericp/uw-nlp-capstone](https://github.com/dericp/uw-nlp-capstone).

There are three potential projects we are excited about.

#### Alternative ELMo Objective

The ELMo training objective is to predict the next word given the history.
However, it's not obvious that this training objective will result in a rich
representation of the input text.  We are curious how ELMo embeddings will
change if we modify the objective to jointly predict the next N words given the
history.  Intuitively, this will force the model to learn a high-level
representation of the input that can recover more than just the next word.

Modifying the current ELMo architecture to jointly predict the next N words
should be rather straightforward. It will be tricky to perform fine-tuning
and run experiments to understand what this modified objective accomplishes.
In the best case, we will discover that modifying the ELMo training objective
to predict more words in the future improves the representations generated by
ELMo and results in performance gains on downstream tasks.

#### Are Parsers Overfit to PTB?

Blah blah blah.

#### Biasing models with Human Attention

For this task, we would start with obtaining datasets with annotated human
attention. We then develop models that can read in human attention and use them
as parameters, mostly by modifying existing models that do not use human attention.
After we trained these models that use human attention, they can be compared
on the same datasets against the models that do not use human attention. We
can then analyze how using human attention can affect existing models. One
possible stretch goal for this project is to beat existing models not using
human attention by certain margin on the same dataset.
